{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RfXefwq0Sbsy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glk5w_ZnPoq8",
        "outputId": "5f37d38c-6adb-45e4-ba54-8a3014bf64f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping from strings to integers for predictions\n",
        "lab = {'reggae':0,\n",
        "       'rock':1,\n",
        "       'country':2,\n",
        "       'disco':3,\n",
        "       'hiphop':4,\n",
        "       'classical':5,\n",
        "       'metal':6,\n",
        "       'blues':7,\n",
        "       'jazz':8,\n",
        "       'pop':9\n",
        "}      \n",
        "nBatch = 6\n",
        "batchDim = list()\n",
        "batchIndex = list()\n",
        "nsong = 100\n",
        "currentIdx = 0\n",
        "for i in range(nBatch):\n",
        "  nBatchSong =int(nsong / nBatch)\n",
        "  nBatch -= 1\n",
        "  batchDim.append(nBatchSong)\n",
        "  nsong -= nBatchSong\n",
        "\n",
        "  nextIdx = currentIdx + nBatchSong -1\n",
        "  idxs = (currentIdx, nextIdx)\n",
        "\n",
        "  currentIdx += nBatchSong\n",
        "  batchIndex.append(idxs)\n",
        "\n",
        "#example with nBatch = 6\n",
        "#batchDim = [17,17,17,17,16,16]\n",
        "#batchIndex =[(0,16),(17,33),(34,50),(51,67),(68,83),(84,99)]\n",
        "# 1) 17songs/genre indexes -> (0-16)\n",
        "# 2) 17songs/genre indexes -> (17-33)\n",
        "# 3) 17songs/genre indexes -> (34-50)\n",
        "# 4) 17songs/genre indexes -> (51-67)\n",
        "# 5) 16songs/genre indexes -> (68-83)\n",
        "# 6) 16songs/genre indexes -> (84-99)\n",
        "\n",
        "def loadBatch(index):\n",
        "  \"\"\"\n",
        "  Samples all the songs applying the sliding window approach and returns two np arrays,\n",
        "  one with all the data extracted and one with the label associated to the segment of the song\n",
        "\n",
        "  param index: the index of the partition of the dataset\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  labels = []\n",
        "  dataset_path = \"/content/drive/MyDrive/genres\"\n",
        "\n",
        "  l = list()\n",
        "\n",
        "  segments_per_track = 21\n",
        "  overlapping = 0.75\n",
        "  track_length = 30 #seconds\n",
        "  sample_length = 5 #seconds\n",
        "\n",
        "  for i,(dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "    if dirpath is not dataset_path:\n",
        "      label = dirpath.split('/')[-1]\n",
        "      print(\"Loading %s\"%label)\n",
        "      for i,f in enumerate(filenames):\n",
        "        if i>=batchIndex[index][0] and i<=batchIndex[index][1]:\n",
        "          file_path = os.path.join(dirpath,f)\n",
        "          signal, sample_rate = librosa.load(file_path)\n",
        "          truncated = 0\n",
        "          if len(signal) < 661500: # if the track is less than 30 seconds i dont take the last segment\n",
        "            l.append(label)\n",
        "            truncated = 1\n",
        "          # Samples all the segments of the song and adds them to an array\n",
        "          for s in range(segments_per_track - truncated ):\n",
        "            start_sample_index = int(s * (1-overlapping) * sample_length * sample_rate) #s = 0 -> 0, s = 1 -> int(27562.5) = 27562\n",
        "            end_sample_index = int(start_sample_index + sample_length * sample_rate-1) #s = 0 -> 27561 , s = 1 -> 55123\n",
        "            sample = signal[start_sample_index:end_sample_index+1]\n",
        "            data.append(sample)\n",
        "            labels.append(lab[label])\n",
        "  return np.array(data),np.array(labels)"
      ],
      "metadata": {
        "id": "SroutCK4dvoE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vYZmvD78pbQj"
      },
      "outputs": [],
      "source": [
        "filter = [(2,128), (6,256), (1,512)] # Defines the sequence of filters to apply to the ANN\n",
        "kernelS = 3\n",
        "poolS = 3\n",
        "stride = [3, 1] # Defines the possible values of the stride to use in the ANN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the selected partition of the dataset for training and validation\n",
        "x_train, y_train=loadBatch(5)\n",
        "x_valid, y_valid=loadBatch(0)"
      ],
      "metadata": {
        "id": "45YxdDMld-ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def unison_shuffled_copies(a, b):\n",
        "    \"\"\"\n",
        "    Generates a random permutation of the value of a and b and applies it to them\n",
        "    \"\"\"\n",
        "    np.random.seed(int(time.time()))\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ],
      "metadata": {
        "id": "9DpS8Ai9DA-a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
        "x_valid, y_valid = unison_shuffled_copies(x_valid, y_valid)"
      ],
      "metadata": {
        "id": "u0CGDEM5Gfwp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "#check if everything correct\n",
        "sample_rate = 22050\n",
        "sf.write('prova.wav', x_train[100,:],sample_rate)\n",
        "\n",
        "print(Counter(y_train))\n",
        "y_train[100]"
      ],
      "metadata": {
        "id": "Wmwj0VMqO5NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSn6WHsS8RgQ"
      },
      "outputs": [],
      "source": [
        "# Residual Layer\n",
        "def res1d(input, nFilters, kernelSize, stride):\n",
        "  \"\"\"\n",
        "  Creates the residual layer and returns it\n",
        "  \n",
        "  param input: the output of the previous layer in the network\n",
        "  param nFilters: the dimension of the filters in the Conv1D\n",
        "  param kernelSize: the dimension of the kernel in Conv1D\n",
        "  param stride: the dimension of the stride in Conv1D\n",
        "  \"\"\"\n",
        "  y = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(input)  \n",
        "  y = keras.layers.BatchNormalization()(y)\n",
        "  y = keras.layers.LeakyReLU()(y)\n",
        "  y = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(y)\n",
        "  y = keras.layers.BatchNormalization()(y)\n",
        "  # if the shape of the shortcut and y aren't equal, we add a convolutional1D layer and a batch normalization to the shortcut\n",
        "  if input.shape[2] != y.shape[2]:\n",
        "    shortcut = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(input)  \n",
        "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
        "  else:\n",
        "    shortcut = input\n",
        "  y = keras.layers.Add()([shortcut, y])\n",
        "  y = keras.layers.LeakyReLU()(y)\n",
        "  return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt8_B1Omo0wP"
      },
      "outputs": [],
      "source": [
        "# Creation of the Input Layer:\n",
        "x = keras.Input(shape=(110250,1))\n",
        "\n",
        "# Creation of the first Convolutional Layer:\n",
        "y = keras.layers.Conv1D(filters=filter[0][1], kernel_size = kernelS, strides = stride[0], padding = \"same\")(x)\n",
        "\n",
        "# Creation of all the Residual layers and the maxPooling\n",
        "for filterType in filter:\n",
        "  for i in range(filterType[0]):\n",
        "    y = res1d(y, filterType[1], kernelS, stride[1])\n",
        "    print(y.shape)\n",
        "    y = keras.layers.MaxPooling1D(pool_size = poolS, strides = stride[0])(y)\n",
        "    print(y.shape)\n",
        "\n",
        "# Creation of the last convolutional layer\n",
        "y = keras.layers.Conv1D(filters=filter[2][1], kernel_size = 1, strides = stride[1], padding = \"same\")(y)\n",
        "\n",
        "# Creation of the of the output layer with activation function \"Softmax\"\n",
        "y = keras.layers.Flatten()(y)\n",
        "predictions = keras.layers.Dense(10, activation='softmax')(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = keras.Model(inputs=x, outputs=predictions)\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Training6batches/5Train0Valid/model.h5\")"
      ],
      "metadata": {
        "id": "TRZhriDPecVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiles the model assigning a loss funciton , optimizer and a metric\n",
        "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer='adam', metrics=keras.metrics.sparse_categorical_accuracy)\n"
      ],
      "metadata": {
        "id": "egiREYA9gvne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "nOzRs2Rhh1Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 4)\n",
        "callbacks_list = [early_stop]"
      ],
      "metadata": {
        "id": "dCmVEVi3d73Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "n_batch = 32\n",
        "# Trains the model  \n",
        "model.fit(x_train, y_train,validation_data = (x_valid, y_valid), epochs=n_epochs, batch_size=n_batch, callbacks=callbacks_list )"
      ],
      "metadata": {
        "id": "Z8Hk15A6gyRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves the trained model in a file\n",
        "model.save(\"/content/drive/MyDrive/Training6batches/5Train0Valid/model.h5\")"
      ],
      "metadata": {
        "id": "E9T1c0v8nVQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ee7fb8-72a5-413e-9504-27ec5e3129de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test=loadBatch(5)"
      ],
      "metadata": {
        "id": "2bBwO1G-woy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad12c29-dfe5-446e-8f4e-7f5bf2d348d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading reggae\n",
            "Loading rock\n",
            "Loading country\n",
            "Loading disco\n",
            "Loading hiphop\n",
            "Loading classical\n",
            "Loading metal\n",
            "Loading blues\n",
            "Loading jazz\n",
            "Loading pop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model.predict(x_test)"
      ],
      "metadata": {
        "id": "EFwppcUy2xWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "accuracy_test = keras.metrics.SparseCategoricalAccuracy()(y_test,y_hat)\n",
        "print('Accuracy (test dataset):%1.2f%%'% (accuracy_test * 100))"
      ],
      "metadata": {
        "id": "oEd1qK9cwaDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdc8b09-b10d-49d2-82c5-3e4d9c62d84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (test dataset):87.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregation of the segments in the songs\n",
        "song=list()\n",
        "for genre in range(10):\n",
        "  seg = y_hat[y_test==genre,:]\n",
        "  pos = 0\n",
        "  resto = len(seg)%21\n",
        "  for i in range(int(len(seg)/21)):\n",
        "    counts = np.mean(seg[pos*21:pos*21 + 21],axis=0)\n",
        "    pos += 1\n",
        "    song.append(counts)\n",
        "  if resto > 0:\n",
        "    counts = np.mean(seg[pos*21:],axis=0)\n",
        "    song.append(counts)\n",
        "\n",
        "\n",
        "songTrue=list()\n",
        "for genre in range(10):\n",
        "  seg = y_test[y_test==genre]\n",
        "  pos = 0\n",
        "  resto = len(seg)%21\n",
        "  for i in range(int(len(seg)/21)):\n",
        "    counts = np.bincount(seg[pos*21:pos*21 + 21])\n",
        "    pos += 1\n",
        "    songTrue.append(np.argmax(counts))\n",
        "  if resto > 0:\n",
        "    counts = np.bincount(seg[pos*21:])\n",
        "    songTrue.append(np.argmax(counts))"
      ],
      "metadata": {
        "id": "O5P69nc7L6nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Aggregate\n",
        "accuracy_test = keras.metrics.SparseCategoricalAccuracy()(songTrue,song)\n",
        "print('Accuracy (test dataset):%1.2f%%'% (accuracy_test * 100))"
      ],
      "metadata": {
        "id": "_-MG6_TeHEtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23e43f3-b372-47cc-ca1e-742ee814aa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (test dataset):90.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NormalModelNAML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}