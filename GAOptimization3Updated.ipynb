{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RfXefwq0Sbsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "sFMNVmbEkN1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Optimizer\n",
        "import keras.backend as K\n",
        "class AccumOptimizer(Optimizer):\n",
        "    \"\"\"Inheriting Optimizer class, wrapping the original optimizer\n",
        "    to achieve a new corresponding optimizer of gradient accumulation.\n",
        "    # Arguments\n",
        "        optimizer: an instance of keras optimizer (supporting\n",
        "                    all keras optimizers currently available);\n",
        "        steps_per_update: the steps of gradient accumulation\n",
        "    # Returns\n",
        "        a new keras optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, steps_per_update=1, **kwargs):\n",
        "        super(AccumOptimizer, self).__init__(**kwargs)\n",
        "        self.optimizer = optimizer\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.steps_per_update = steps_per_update\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.cond = K.equal(self.iterations % self.steps_per_update, 0)\n",
        "            self.lr = self.optimizer.lr\n",
        "            self.optimizer.lr = K.switch(self.cond, self.optimizer.lr, 0.)\n",
        "            for attr in ['momentum', 'rho', 'beta_1', 'beta_2']:\n",
        "                if hasattr(self.optimizer, attr):\n",
        "                    value = getattr(self.optimizer, attr)\n",
        "                    setattr(self, attr, value)\n",
        "                    setattr(self.optimizer, attr, K.switch(self.cond, value, 1 - 1e-7))\n",
        "            for attr in self.optimizer.get_config():\n",
        "                if not hasattr(self, attr):\n",
        "                    value = getattr(self.optimizer, attr)\n",
        "                    setattr(self, attr, value)\n",
        "            # Cover the original get_gradients method with accumulative gradients.\n",
        "            def get_gradients(loss, params):\n",
        "                return [ag / self.steps_per_update for ag in self.accum_grads]\n",
        "            self.optimizer.get_gradients = get_gradients\n",
        "    def get_updates(self, loss, params):\n",
        "        self.updates = [\n",
        "            K.update_add(self.iterations, 1),\n",
        "            K.update_add(self.optimizer.iterations, K.cast(self.cond, 'int64')),\n",
        "        ]\n",
        "        # gradient accumulation\n",
        "        self.accum_grads = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        for g, ag in zip(grads, self.accum_grads):\n",
        "            self.updates.append(K.update(ag, K.switch(self.cond, ag * 0, ag + g)))\n",
        "        # inheriting updates of original optimizer\n",
        "        self.updates.extend(self.optimizer.get_updates(loss, params)[1:])\n",
        "        self.weights.extend(self.optimizer.weights)\n",
        "        return self.updates\n",
        "    def get_config(self):\n",
        "        iterations = K.eval(self.iterations)\n",
        "        K.set_value(self.iterations, 0)\n",
        "        config = self.optimizer.get_config()\n",
        "        K.set_value(self.iterations, iterations)\n",
        "        return config"
      ],
      "metadata": {
        "id": "rzT5wL0XQonN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glk5w_ZnPoq8",
        "outputId": "549aa33c-f4f4-423c-92e6-44b7c595da44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab = {'reggae':0,\n",
        "       'rock':1,\n",
        "       'country':2,\n",
        "       'disco':3,\n",
        "       'hiphop':4,\n",
        "       'classical':5,\n",
        "       'metal':6,\n",
        "       'blues':7,\n",
        "       'jazz':8,\n",
        "       'pop':9\n",
        "}      \n",
        "nBatch = 3\n",
        "batchDim = list()\n",
        "batchIndex = list()\n",
        "nsong = 100\n",
        "currentIdx = 0\n",
        "for i in range(nBatch):\n",
        "  nBatchSong =int(nsong / nBatch)\n",
        "  nBatch -= 1\n",
        "  batchDim.append(nBatchSong)\n",
        "  nsong -= nBatchSong\n",
        "\n",
        "  nextIdx = currentIdx + nBatchSong -1\n",
        "  idxs = (currentIdx, nextIdx)\n",
        "\n",
        "  currentIdx += nBatchSong\n",
        "  batchIndex.append(idxs)\n",
        "\n",
        "#example with nBatch = 6\n",
        "#batchDim = [17,17,17,17,16,16]\n",
        "#batchIndex =[(0,16),(17,33),(34,50),(51,67),(68,83),(84,99)]\n",
        "# 1) 17songs/genre indexes -> (0-16)\n",
        "# 2) 17songs/genre indexes -> (17-33)\n",
        "# 3) 17songs/genre indexes -> (34-50)\n",
        "# 4) 17songs/genre indexes -> (51-67)\n",
        "# 5) 16songs/genre indexes -> (68-83)\n",
        "# 6) 16songs/genre indexes -> (84-99)\n",
        "\n",
        "def loadBatch(index):\n",
        "  data = []\n",
        "  labels = []\n",
        "  dataset_path = \"/content/drive/MyDrive/genres\"\n",
        "\n",
        "  l = list()\n",
        "\n",
        "  segments_per_track = 21\n",
        "  overlapping = 0.75\n",
        "  track_length = 30 #seconds\n",
        "  sample_length = 5 #seconds\n",
        "\n",
        "  for i,(dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "    if dirpath is not dataset_path:\n",
        "      label = dirpath.split('/')[-1]\n",
        "      print(\"Loading %s\"%label)\n",
        "      for i,f in enumerate(filenames):\n",
        "        if i>=batchIndex[index][0] and i<=batchIndex[index][1]:\n",
        "          file_path = os.path.join(dirpath,f)\n",
        "          signal, sample_rate = librosa.load(file_path)\n",
        "          truncated = 0\n",
        "          if len(signal) < 661500: # if the track is less than 30 seconds i dont take the last segment\n",
        "            l.append(label)\n",
        "            truncated = 1\n",
        "          for s in range(segments_per_track - truncated ):\n",
        "            start_sample_index = int(s * (1-overlapping) * sample_length * sample_rate) #s = 0 -> 0, s = 1 -> int(27562.5) = 27562\n",
        "            end_sample_index = int(start_sample_index + sample_length * sample_rate-1) #s = 0 -> 27561 , s = 1 -> 55123\n",
        "            sample = signal[start_sample_index:end_sample_index+1]\n",
        "            data.append(sample)\n",
        "            labels.append(lab[label])\n",
        "  return np.array(data),np.array(labels)"
      ],
      "metadata": {
        "id": "SroutCK4dvoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchDim, batchIndex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt6NIeVtDE8l",
        "outputId": "63b85223-fdf4-4969-dc2d-c36c9080fb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([33, 33, 34], [(0, 32), (33, 65), (66, 99)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYZmvD78pbQj"
      },
      "outputs": [],
      "source": [
        "filter = [(2,128), (6,256), (1,512)]\n",
        "kernelS = 3\n",
        "poolS = 3\n",
        "stride = [3, 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train=loadBatch(0)\n",
        "x_valid, y_valid=loadBatch(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45YxdDMld-ix",
        "outputId": "232b2ef0-a777-4fab-b9fd-65599c7d3fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading reggae\n",
            "Loading rock\n",
            "Loading country\n",
            "Loading disco\n",
            "Loading hiphop\n",
            "Loading classical\n",
            "Loading metal\n",
            "Loading blues\n",
            "Loading jazz\n",
            "Loading pop\n",
            "Loading reggae\n",
            "Loading rock\n",
            "Loading country\n",
            "Loading disco\n",
            "Loading hiphop\n",
            "Loading classical\n",
            "Loading metal\n",
            "Loading blues\n",
            "Loading jazz\n",
            "Loading pop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmRRBWQJGEnw",
        "outputId": "ff086bf3-08e7-4ba0-c9f7-8ef133349eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6925, 110250)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def unison_shuffled_copies(a, b):\n",
        "    np.random.seed(int(time.time()))\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ],
      "metadata": {
        "id": "9DpS8Ai9DA-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
        "x_valid, y_valid = unison_shuffled_copies(x_valid, y_valid)"
      ],
      "metadata": {
        "id": "u0CGDEM5Gfwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "#check if everything correct\n",
        "sample_rate = 22050\n",
        "sf.write('prova.wav', x_train[100,:],sample_rate)\n",
        "\n",
        "print(Counter(y_train))\n",
        "y_train[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmwj0VMqO5NO",
        "outputId": "a2f39731-17ea-43ad-d13a-6a262d6dd242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({8: 693, 0: 693, 5: 693, 6: 693, 1: 693, 9: 693, 7: 693, 3: 692, 4: 692, 2: 690})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSn6WHsS8RgQ"
      },
      "outputs": [],
      "source": [
        "# Residual Layer\n",
        "def res1d(input, nFilters, kernelSize, stride):\n",
        "  y = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(input)  \n",
        "  y = keras.layers.BatchNormalization()(y)\n",
        "  y = keras.layers.LeakyReLU()(y)\n",
        "  y = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(y)\n",
        "  y = keras.layers.BatchNormalization()(y)\n",
        "  # if the shape of the shortcut and y aren't equal, we add a convolutional1D layer and a batch normalization to the shortcut\n",
        "  if input.shape[2] != y.shape[2]:\n",
        "    shortcut = keras.layers.Conv1D(filters = nFilters, kernel_size = kernelSize, strides = stride, padding = \"same\")(input)  \n",
        "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
        "  else:\n",
        "    shortcut = input\n",
        "  y = keras.layers.Add()([shortcut, y])\n",
        "  y = keras.layers.LeakyReLU()(y)\n",
        "  return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt8_B1Omo0wP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ecfadf-843d-48d1-9c46-0d4740e6f0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "(None, 36750, 128)\n",
            "(None, 12250, 128)\n",
            "(None, 12250, 128)\n",
            "(None, 4083, 128)\n",
            "(None, 4083, 256)\n",
            "(None, 1361, 256)\n",
            "(None, 1361, 256)\n",
            "(None, 453, 256)\n",
            "(None, 453, 256)\n",
            "(None, 151, 256)\n",
            "(None, 151, 256)\n",
            "(None, 50, 256)\n",
            "(None, 50, 256)\n",
            "(None, 16, 256)\n",
            "(None, 16, 256)\n",
            "(None, 5, 256)\n",
            "(None, 5, 512)\n",
            "(None, 1, 512)\n"
          ]
        }
      ],
      "source": [
        "x = keras.Input(shape=(110250,1))\n",
        "\n",
        "#First Convolutional Layer:\n",
        "y = keras.layers.Conv1D(filters=filter[0][1], kernel_size = kernelS, strides = stride[0], padding = \"same\")(x)\n",
        "\n",
        "#Series of Residual Layers and MaxPools:\n",
        "for filterType in filter:\n",
        "  for i in range(filterType[0]):\n",
        "    y = res1d(y, filterType[1], kernelS, stride[1])\n",
        "    print(y.shape)\n",
        "    y = keras.layers.MaxPooling1D(pool_size = poolS, strides = stride[0])(y)\n",
        "    print(y.shape)\n",
        "\n",
        "#Last convolutional layer\n",
        "y = keras.layers.Conv1D(filters=filter[2][1], kernel_size = 1, strides = stride[1], padding = \"same\")(y)\n",
        "\n",
        "#Last layers for output\n",
        "y = keras.layers.Flatten()(y)\n",
        "predictions = keras.layers.Dense(10, activation='softmax')(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Model(inputs=x, outputs=predictions)\n",
        "#model.load_weights(\"/content/drive/MyDrive/GATraining/modelWeights23.h5\")"
      ],
      "metadata": {
        "id": "TRZhriDPecVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizer_v1 import adam\n",
        "\n",
        "opt = AccumOptimizer(adam(), 8) # 8 is accumulative steps"
      ],
      "metadata": {
        "id": "hhryjM5TPgJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling \n",
        "model.compile(loss = [keras.losses.SparseCategoricalCrossentropy()],\n",
        "              optimizer=opt, metrics=[keras.metrics.sparse_categorical_accuracy])\n"
      ],
      "metadata": {
        "id": "egiREYA9gvne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOzRs2Rhh1Ed",
        "outputId": "a3e204cc-773f-47e1-c491-b8b71ebcd45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 110250, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 36750, 128)   512         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 36750, 128)   49280       ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 36750, 128)  512         ['conv1d_1[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 36750, 128)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 36750, 128)   49280       ['leaky_re_lu[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 36750, 128)  512         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 36750, 128)   0           ['conv1d[0][0]',                 \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 36750, 128)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 12250, 128)   0           ['leaky_re_lu_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 12250, 128)   49280       ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 12250, 128)  512         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 12250, 128)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 12250, 128)   49280       ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 12250, 128)  512         ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 12250, 128)   0           ['max_pooling1d[0][0]',          \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 12250, 128)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 4083, 128)   0           ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 4083, 256)    98560       ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 4083, 256)   1024        ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)      (None, 4083, 256)    0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 4083, 256)    98560       ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 4083, 256)    196864      ['leaky_re_lu_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 4083, 256)   1024        ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 4083, 256)   1024        ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 4083, 256)    0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 4083, 256)    0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 1361, 256)   0           ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 1361, 256)    196864      ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 1361, 256)   1024        ['conv1d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)      (None, 1361, 256)    0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 1361, 256)    196864      ['leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 1361, 256)   1024        ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 1361, 256)    0           ['max_pooling1d_2[0][0]',        \n",
            "                                                                  'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 1361, 256)    0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 453, 256)    0           ['leaky_re_lu_7[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 453, 256)     196864      ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 453, 256)    1024        ['conv1d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 453, 256)     0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 453, 256)     196864      ['leaky_re_lu_8[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 453, 256)    1024        ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 453, 256)     0           ['max_pooling1d_3[0][0]',        \n",
            "                                                                  'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 453, 256)     0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 151, 256)    0           ['leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 151, 256)     196864      ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 151, 256)    1024        ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_10 (LeakyReLU)     (None, 151, 256)     0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 151, 256)     196864      ['leaky_re_lu_10[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 151, 256)    1024        ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 151, 256)     0           ['max_pooling1d_4[0][0]',        \n",
            "                                                                  'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_11 (LeakyReLU)     (None, 151, 256)     0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 50, 256)     0           ['leaky_re_lu_11[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 50, 256)      196864      ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 50, 256)     1024        ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_12 (LeakyReLU)     (None, 50, 256)      0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 50, 256)      196864      ['leaky_re_lu_12[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 50, 256)     1024        ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 50, 256)      0           ['max_pooling1d_5[0][0]',        \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_13 (LeakyReLU)     (None, 50, 256)      0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 16, 256)     0           ['leaky_re_lu_13[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 16, 256)      196864      ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 256)     1024        ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_14 (LeakyReLU)     (None, 16, 256)      0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 16, 256)      196864      ['leaky_re_lu_14[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 256)     1024        ['conv1d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 16, 256)      0           ['max_pooling1d_6[0][0]',        \n",
            "                                                                  'batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_15 (LeakyReLU)     (None, 16, 256)      0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 5, 256)      0           ['leaky_re_lu_15[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 5, 512)       393728      ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 5, 512)      2048        ['conv1d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_16 (LeakyReLU)     (None, 5, 512)       0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 5, 512)       393728      ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 5, 512)       786944      ['leaky_re_lu_16[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 5, 512)      2048        ['conv1d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 5, 512)      2048        ['conv1d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 512)       0           ['batch_normalization_19[0][0]', \n",
            "                                                                  'batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " leaky_re_lu_17 (LeakyReLU)     (None, 5, 512)       0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 1, 512)      0           ['leaky_re_lu_17[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 1, 512)       262656      ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['conv1d_21[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           5130        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,423,946\n",
            "Trainable params: 4,413,194\n",
            "Non-trainable params: 10,752\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5)\n",
        "callbacks_list = [early_stop]"
      ],
      "metadata": {
        "id": "dCmVEVi3d73Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "n_batch = 10\n",
        "\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "x_valid = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], 1))\n",
        "# Fitting \n",
        "model.fit(x_train, y_train,validation_data = (x_valid, y_valid), epochs=n_epochs, batch_size=n_batch, callbacks=callbacks_list )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Hk15A6gyRG",
        "outputId": "c3fffb61-286e-4496-9046-eb4fd1083184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 6925 samples, validate on 6926 samples\n",
            "Epoch 1/100\n",
            "6925/6925 [==============================] - ETA: 0s - loss: 1.7086 - sparse_categorical_accuracy: 0.5175"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6925/6925 [==============================] - 635s 92ms/sample - loss: 1.7086 - sparse_categorical_accuracy: 0.5175 - val_loss: 2.2588 - val_sparse_categorical_accuracy: 0.4215\n",
            "Epoch 2/100\n",
            "6925/6925 [==============================] - 620s 90ms/sample - loss: 0.6983 - sparse_categorical_accuracy: 0.7669 - val_loss: 2.2633 - val_sparse_categorical_accuracy: 0.4600\n",
            "Epoch 3/100\n",
            "6925/6925 [==============================] - 621s 90ms/sample - loss: 0.4529 - sparse_categorical_accuracy: 0.8503 - val_loss: 3.0508 - val_sparse_categorical_accuracy: 0.4394\n",
            "Epoch 4/100\n",
            "6925/6925 [==============================] - 621s 90ms/sample - loss: 0.3486 - sparse_categorical_accuracy: 0.8868 - val_loss: 2.7049 - val_sparse_categorical_accuracy: 0.4190\n",
            "Epoch 5/100\n",
            "6925/6925 [==============================] - 620s 90ms/sample - loss: 0.2537 - sparse_categorical_accuracy: 0.9162 - val_loss: 3.3593 - val_sparse_categorical_accuracy: 0.4242\n",
            "Epoch 6/100\n",
            "6920/6925 [============================>.] - ETA: 0s - loss: 0.2191 - sparse_categorical_accuracy: 0.9251"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/GATraining/modelWeights34.h5\")"
      ],
      "metadata": {
        "id": "E9T1c0v8nVQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GAOptimization3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}